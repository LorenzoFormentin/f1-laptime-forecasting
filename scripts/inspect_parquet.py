from __future__ import annotations

import argparse
from pathlib import Path

import pandas as pd


def _print_df_info(df: pd.DataFrame, name: str, max_cols: int = 40) -> None:
    print("\n" + "=" * 80)
    print(f"{name}")
    print("=" * 80)

    print(f"Shape: {df.shape}")
    print("\nDtypes:")
    # print dtypes in a stable order
    dtypes = df.dtypes.astype(str)
    if len(dtypes) > max_cols:
        print(dtypes.head(max_cols).to_string())
        print(f"... ({len(dtypes) - max_cols} more columns)")
    else:
        print(dtypes.to_string())

    print("\nHead (10):")
    with pd.option_context("display.max_columns", 200, "display.width", 160):
        print(df.head(10))

    # Useful quick checks if present
    if "LapTime_s" in df.columns:
        print("\nLapTime_s summary:")
        print(df["LapTime_s"].describe())

    if "LapTime_next_s" in df.columns:
        print("\nLapTime_next_s summary:")
        print(df["LapTime_next_s"].describe())
        
    # ---- QA / sanity checks (best-effort, no hard assumptions) ----
    key_cols = [c for c in ["Year", "EventName", "Session", "Driver", "LapNumber"] if c in df.columns]
    if key_cols:
        dup_keys = int(df.duplicated(subset=key_cols).sum())
        print(f"\nDuplicate keys on {key_cols}: {dup_keys}")

    dup_rows = int(df.duplicated().sum())
    print(f"Duplicate full rows: {dup_rows}")

    nulls = df.isna().sum().sort_values(ascending=False)
    top_nulls = nulls[nulls > 0].head(15)
    if len(top_nulls) > 0:
        print("\nTop missing columns:")
        print(top_nulls.to_string())
    else:
        print("\nNo missing values detected.")

    if "LapTime_s" in df.columns and pd.api.types.is_numeric_dtype(df["LapTime_s"]):
        bad = int((df["LapTime_s"] <= 0).sum())
        if bad:
            print(f"\nWARNING: LapTime_s has {bad} non-positive values")

    if (
        "LapTime_s" in df.columns
        and "LapTime_next_s" in df.columns
        and pd.api.types.is_numeric_dtype(df["LapTime_s"])
        and pd.api.types.is_numeric_dtype(df["LapTime_next_s"])
    ):
        delta = (df["LapTime_next_s"] - df["LapTime_s"]).abs()
        extreme = int((delta > 30).sum())
        if extreme:
            print(f"WARNING: {extreme} rows have |next-current| > 30s (check pit/flags policy)")



def main() -> None:
    p = argparse.ArgumentParser(description="Inspect parquet datasets generated by the pipeline")
    p.add_argument(
        "--path",
        type=str,
        default="",
        help="Path to a parquet file. If omitted, the script will inspect the latest files in data/interim and data/processed.",
    )
    p.add_argument("--data-dir", type=str, default="data", help="Project data directory (default: data)")
    args = p.parse_args()

    data_dir = Path(args.data_dir)

    if args.path:
        path = Path(args.path)
        if not path.exists():
            raise SystemExit(f"File not found: {path}")
        df = pd.read_parquet(path)
        _print_df_info(df, name=str(path))
        return

    # Otherwise: find the latest in interim and processed
    interim = sorted((data_dir / "interim").glob("*.parquet"), key=lambda p: p.stat().st_mtime)
    processed = sorted((data_dir / "processed").glob("*.parquet"), key=lambda p: p.stat().st_mtime)

    if interim:
        latest_interim = interim[-1]
        df_i = pd.read_parquet(latest_interim)
        _print_df_info(df_i, name=f"Latest interim: {latest_interim}")
    else:
        print("No interim parquet files found in data/interim/")

    if processed:
        latest_processed = processed[-1]
        df_p = pd.read_parquet(latest_processed)
        _print_df_info(df_p, name=f"Latest processed: {latest_processed}")
    else:
        print("No processed parquet files found in data/processed/")


if __name__ == "__main__":
    main()
